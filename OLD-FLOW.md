# ğŸš€ CCTV Streaming + AI Recognition System (Production Architecture)

This document explains:

1. Our **current CCTV streaming architecture**
2. Why it works for **1â€“6 cameras**
3. Why it **fails beyond ~20 cameras**
4. Why we are migrating to **LiveKit (WebRTC media server)**
5. What changes and what stays the same

This is **not a college demo** â€” this design is meant for real deployments.

---

## ğŸ§  Current System Overview

### High-level flow (current)
- RTSP Camera
-   â†“
- FFmpeg (spawned by Node)
-   â†“ 
- MPEG-TS (mpeg1video)
-   â†“
- Node.js
-   â†“ 
- WebSocket
-   â†“
- Browser (JSMpeg)



### Frontend
- Receives **video stream via WebSocket**
- Decodes using **JSMpeg (WASM)**
- Draws bounding boxes on overlay canvas

### Node.js
- Spawns FFmpeg per camera
- Pipes video bytes through Node
- Broadcasts video to WebSocket clients
- Receives AI events via Redis
- Writes logs to DB
- Sends metadata overlays to frontend

### Python AI
- Reads RTSP directly
- Detects faces
- Tracks persons
- Publishes events to Redis

---

## âœ… Why This Works for 1â€“6 Cameras

This setup works **initially** because:

### 1ï¸âƒ£ Low concurrency
- Few FFmpeg processes
- Few WebSocket clients
- Node event loop not saturated

### 2ï¸âƒ£ Manageable CPU load
- MPEG1 decoding happens in browser
- Node only relays bytes
- Memory usage stays bounded

### 3ï¸âƒ£ Acceptable latency
- ~200â€“400 ms latency
- Fine for demos, POCs, internal testing

### 4ï¸âƒ£ Simple deployment
- Single Node server
- No separate media infrastructure
- Easy to debug early on

For **small installations (1â€“6 cameras, 1â€“2 viewers)** this is acceptable.

---

## âŒ Why This Architecture Fails at ~20 Cameras

As camera count increases, **fundamental architectural limits** are hit.

---

### âŒ Problem 1: Node.js becomes a media server (wrong role)

Node is handling:
- raw video bytes
- fan-out to multiple clients
- buffering
- backpressure (poorly)

Node is **not designed** to be a media data plane.

**Result:**
- event loop stalls
- GC pauses
- dropped frames
- random freezes

---

### âŒ Problem 2: No congestion control

WebSocket streaming:
- has no adaptive bitrate
- has no packet loss recovery
- has no jitter buffer

One slow client can:
- block buffers
- increase memory usage
- affect all viewers

---

### âŒ Problem 3: CPU-heavy frontend decoding

JSMpeg:
- decodes video in JavaScript/WASM
- no hardware acceleration
- very expensive at scale

At ~20 cameras:
- laptops throttle
- mobile devices fail
- browsers drop frames

---

### âŒ Problem 4: Poor scalability model

Scaling WebSocket video means:
- more Node instances
- more FFmpeg processes
- duplicated RTSP pulls
- higher camera load

This does **not scale horizontally**.

---

### âŒ Problem 5: Unreliable long uptime

Over hours/days:
- FFmpeg restarts accumulate
- memory slowly grows
- streams desync from AI overlays
- system requires manual restarts

This is unacceptable for production CCTV.

---

## ğŸ§¨ Root Cause Summary

| Issue | Reason |
|-----|------|
Node instability | Media handled in JS |
High CPU | MPEG1 + JSMpeg |
No backpressure | WebSocket misuse |
No scaling | Tight coupling |
Poor reliability | No media protocol |

---

## âœ… Why LiveKit Fixes All These Problems

### Key architectural change

**Node no longer handles video bytes.**

---

## ğŸŸ© New Architecture (with LiveKit)

RTSP Camera
â†“
FFmpeg (RTSP â†’ RTP)
â†“
LiveKit (WebRTC SFU)
â†“
Browser <video>

Python AI
â†“ Redis
Node.js
â†“ WebSocket
Browser (bbox overlays)



---

## ğŸ§© Responsibilities After Migration

### LiveKit (Media Server)
- WebRTC streaming
- Jitter buffering
- Packet loss recovery
- Congestion control
- Hardware decoding in browser
- Multi-client fan-out

### Node.js (Control Plane)
- Authentication
- Camera registry
- Start/stop FFmpeg
- LiveKit signaling
- AI event processing
- DB writes
- WebSocket metadata

### Python AI
- Unchanged
- Continues RTSP processing
- Publishes recognition events

---

## ğŸš€ Why This Scales to 20+ Cameras

### âœ… Media offloaded from Node
- Node handles **control**, not data
- Stable under load

### âœ… WebRTC is designed for video
- Adaptive bitrate
- NAT traversal
- Hardware decoding
- Low latency

### âœ… One FFmpeg per camera
- No duplicate RTSP pulls
- Camera-friendly

### âœ… Browser performance
- GPU accelerated decoding
- Works on mobile & desktop

### âœ… Horizontal scaling
- LiveKit scales independently
- Node scales independently
- AI workers scale independently

---

## ğŸ“ˆ Expected Performance (Realistic)

| Cameras | Result |
|-------|-------|
1â€“6 | Perfect |
10â€“20 | Stable |
20â€“50 | Scales with infra |
50+ | Add nodes / SFU |

---

## ğŸ§  Final Design Principles (Non-negotiable)

- **RTSP â‰  Browser protocol**
- **WebSocket â‰  Video transport**
- **Node â‰  Media server**
- **WebRTC = Real-time video**
- **WS = Metadata only**

---

## ğŸ Conclusion

The original WebSocket + JSMpeg setup is:
- good for learning
- good for demos
- NOT production-grade

Migrating to **LiveKit**:
- preserves your AI pipeline
- preserves your overlay logic
- fixes scaling and reliability
- enables real deployments

This is the architecture used by:
- enterprise VMS
- smart offices
- airports
- access-control systems

---

## âœ… Next Steps

1. Deploy LiveKit server
2. Route RTSP â†’ FFmpeg â†’ LiveKit
3. Replace JSMpeg with WebRTC player
4. Keep WebSocket overlays unchanged
5. Gradually remove WS video streaming



